{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Introduction\n",
    "\n",
    "Welcome to the introduction of PyTorch. PyTorch is a scientific computing package targeted for two main purposes: \n",
    "\n",
    "1. A replacement for NumPy with the ability to use the power of GPUs.\n",
    "\n",
    "2. A deep learning framework that enables the flexible and swift building of neural network models.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "\n",
    "### Goals of this tutorial\n",
    "\n",
    "- Understanding PyTorch's Tensor and neural networks libraries at an overview level.\n",
    "\n",
    "- Training a neural network using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Mount folder in Colab\n",
    "\n",
    "Uncomment thefollowing cell to mount your gdrive if you are using the notebook in google colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom google.colab import drive\\nimport os\\n\\ngdrive_path='/content/gdrive/MyDrive/i2dl/exercise_07'\\n\\n# This will mount your google drive under 'MyDrive'\\ndrive.mount('/content/gdrive', force_remount=True)\\n# In order to access the files in this notebook we have to navigate to the correct folder\\nos.chdir(gdrive_path)\\n# Check manually if all files are present\\nprint(sorted(os.listdir()))\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "gdrive_path='/content/gdrive/MyDrive/i2dl/exercise_07'\n",
    "\n",
    "# This will mount your google drive under 'MyDrive'\n",
    "drive.mount('/content/gdrive', force_remount=True)\n",
    "# In order to access the files in this notebook we have to navigate to the correct folder\n",
    "os.chdir(gdrive_path)\n",
    "# Check manually if all files are present\n",
    "print(sorted(os.listdir()))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable GPUs on Colab\n",
    "\n",
    "Having a library that has GPU support is one thing, the other is actually owning the hardware. Alternatively, you can use google colab though we have to manually enable it.\n",
    "\n",
    "To enable GPU support in Google Colab go to `Menu > Runtime > Change runtime type` and enable the GPU hardware accelerator to speed up your trainings considerably. However, this functionality might not be available at any time.\n",
    "\n",
    "<img src=\"./images/colab_gpu.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Installing PyTorch\n",
    "\n",
    "Pytorch provides support for accelerating computation using CUDA enabled GPU's. If your workstation has an NVIDIA GPU, install PyTorch along with the CUDA component.\n",
    "\n",
    "#### Install [PyTorch](https://pytorch.org/) and [torchvision](https://github.com/pytorch/vision)\n",
    "\n",
    "For this class we will use the current Pytorch version 1.11. To install, please uncomment and run the proper line in the upcoming cell depending on your operating system (and CUDA setup). We won't go into details of the installation process.\n",
    "\n",
    "# **Note** (!!!!!!!!!!!):\n",
    "All packages should be installed on your i2dl conda enviroment. Otherwise, you'd start with mismatching versions loops of differnet libraries, which will make your life really misrable later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.11.0+cpu (from versions: 0.4.1, 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.1.0.post2, 1.2.0, 1.3.0, 1.3.0.post2, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.11.0+cpu\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "\n",
    "# For google colab\n",
    "# !python -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "\n",
    "# For Linux and probably Windows (CPU)\n",
    "!{sys.executable} -m pip install torch==1.11.0+cpu torchvision==0.12.0+cpu torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# For Linux and probably Windows (Prerequisites: Nvidia GPU + CUDA toolkit 11.3)\n",
    "# !{sys.executable} -m pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "\n",
    "# For OS X/Mac\n",
    "# !{sys.executable} -m pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Nvidia GPU</b>\n",
    "    <p>If you have a rather recent Nvidia GPU, you can go ahead and install the CUDA toolkit together with a current version of cudnn (though it is possible to use other versions as long as you build it yourself). Afterwards, you can run the respective line in the cell above.</p>\n",
    "    <p>There are multiple setups on how to install those on both Linux and Windows, but it depends on your setup. If you want to utilize your GPU you have to go through those steps. Use the forum for help if you get stuck.\n",
    "    But, google or ChatGPT are your new best friends.</p>\n",
    "    <br>\n",
    "    <b>Google Colab Pytorch Installation Time</b>\n",
    "    <p>Google colab might use an older/newer version of pytorch. Since we are mostly using defualt functionality, you should be fine by using the default colab version to avoid the long installation time at your own risk.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking PyTorch Installation and Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version Installed: 1.13.1\n",
      "Torchvision version Installed: 0.14.1\n",
      "\n",
      "you are using an another version of PyTorch. We expect PyTorch 1.11.0. You may continue using your version but it might cause dependency and compatibility issues.\n",
      "you are using an another version of torchvision. We expect torchvision 0.12.0. You can continue with your version but it might cause dependency and compatibility issues.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(f\"PyTorch version Installed: {torch.__version__}\\nTorchvision version Installed: {torchvision.__version__}\\n\")\n",
    "if not torch.__version__.startswith(\"1.11\"):\n",
    "    print(\"you are using an another version of PyTorch. We expect PyTorch 1.11.0. You may continue using your version but it\"\n",
    "          \" might cause dependency and compatibility issues.\")\n",
    "if not torchvision.__version__.startswith(\"0.12\"):\n",
    "    print(\"you are using an another version of torchvision. We expect torchvision 0.12.0. You can continue with your version but it\"\n",
    "          \" might cause dependency and compatibility issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the end of installation. Let's dive right into PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Getting Started\n",
    "\n",
    "In this section you will learn the basic element Tensor and some simple operations in PyTorch.\n",
    "The following block imports the required packages for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True' # To prevent the kernel from dying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tensors\n",
    "\n",
    "[`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) is the central class of PyTorch.\n",
    "Tensors are similar to NumPyâ€™s ndarrays. The advantage of using Tensors is that one can easily transfer them from CPU to GPU and therefore computations on tensors can be accelerated with a GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.1 Initializing Tensor\n",
    "Let us construct a NumPy array and a tensor of shape (2,3) directly from data values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable array_np:\n",
      "Datatype: <class 'numpy.ndarray'>\n",
      "Shape: (2, 3)\n",
      "Values:\n",
      " [[1 2 3]\n",
      " [5 6 7]]\n",
      "\n",
      "\n",
      "Variable array_ts:\n",
      "Datatype <class 'torch.Tensor'>\n",
      "Shape: torch.Size([2, 3])\n",
      "Values:\n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Initializing the Numpy Array\n",
    "array_np = np.array([[1,2,3],[5,6,7]]) # A NumPy array\n",
    "\n",
    "# Initializing the Tensor\n",
    "array_ts = torch.tensor([[1,2,3],[4,5,6]]) # A Tensor\n",
    "\n",
    "print(\"Variable array_np:\\nDatatype: {}\\nShape: {}\".format(type(array_np), array_np.shape))\n",
    "print(\"Values:\\n\", array_np)\n",
    "print(\"\\n\\nVariable array_ts:\\nDatatype {}\\nShape: {}\".format(type(array_ts), array_ts.shape))\n",
    "print(\"Values:\\n\", array_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.2 Conversion between NumPy array and Tensor\n",
    "\n",
    "The conversion between NumPy ndarray and PyTorch tensor is quite easy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Conversion\n",
    "array_np = np.array([1, 2, 3])\n",
    "\n",
    "# Conversion from  a numpy array to a Tensor\n",
    "array_ts_2 = torch.from_numpy(array_np) \n",
    "\n",
    "# Conversion from  Tensor to numpy array\n",
    "array_np_2 = array_ts_2.numpy() \n",
    "\n",
    "# Change a value of the np_array\n",
    "array_np_2[1] = -1 \n",
    "\n",
    "# Changes in the numpy array will also change the values in the tensor\n",
    "assert(array_np[1] == array_np_2[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><b></b> During the conversion, both ndarray and Tensor share the same memory address. Changes in value of one will\n",
    "affect the other.</div>\n",
    "\n",
    "## 1.3 Operations on Tensor\n",
    "\n",
    "### 1.3.1 Indexing\n",
    "\n",
    "We can use the NumPy array-like indexing for Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2],\n",
      "        [0, 5]])\n"
     ]
    }
   ],
   "source": [
    "# Let us take the first two columns from the original tensor array and save it in a new one\n",
    "b = array_ts[:2, :2] \n",
    "\n",
    "# Let's assign the value of first column of the new variable to be zero \n",
    "b[:, 0] = 0 \n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now select elements which satisfy a particular condition. In this example, let's find those elements of tensor which are array greater than one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# Index of the elements with value greater than one\n",
    "mask = array_ts > 1 \n",
    "new_array = array_ts[mask]\n",
    "print(new_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try performing the same operation in a single line of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "c = array_ts[array_ts>1]\n",
    "\n",
    "# Is the result same as the array from the previous cell?\n",
    "print(c == new_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1.3.2 Mathematical operations on Tensor\n",
    "\n",
    "#### Element-wise operations on Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y: \n",
      "tensor([[ 6,  8],\n",
      "        [10, 12]])\n",
      "x + y: \n",
      "tensor([[ 6,  8],\n",
      "        [10, 12]])\n",
      "x + y: \n",
      "tensor([[ 6.,  8.],\n",
      "        [10., 12.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2],[3,4]])\n",
    "y = torch.tensor([[5,6],[7,8]])\n",
    "\n",
    "# Elementwise Addition of the tensors\n",
    "# [[ 6.0  8.0]\n",
    "#  [10.0 12.0]]\n",
    "\n",
    "# Addition - Syntax 1\n",
    "print(\"x + y: \\n{}\".format(x + y))\n",
    "\n",
    "# Addition - Syntax 2\n",
    "print(\"x + y: \\n{}\".format(torch.add(x, y)))\n",
    "\n",
    "# Addition - Syntax 3\n",
    "result_add = torch.empty(2, 2)\n",
    "torch.add(x, y, out=result_add)\n",
    "print(\"x + y: \\n{}\".format(result_add))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Similar syntax holds for other element-wise operations such as subtraction and multiplication.\n",
    "\n",
    "When dividing two integers in NumPy as well PyTorch, the result is always a **float**.   \n",
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2        0.33333333]\n",
      " [0.42857143 0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "x_np = np.array([[1,2],[3,4]])\n",
    "y_np = np.array([[5,6],[7,8]])\n",
    "print(x_np / y_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Devices\n",
    "\n",
    "When training a neural network, it is important to make sure that all the required tensors as well as the model are on the same device. Tensors can be moved between the CPU and GPU using `.to` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check if a GPU is available. If it is available, we will assign it to `device` and move the tensor `x` to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Original device: cpu\n",
      "Current device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "print(f\"Original device: {x.device}\") # \"cpu\"\n",
    "\n",
    "tensor = x.to(device)\n",
    "print(f\"Current device: {tensor.device}\") #\"cpu\" or \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `x` has been moved on to a CUDA device for those who have a GPU; otherwise it's still on the CPU.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b> Try including the <b>.to(device)</b> calls in your codes. It is then easier to port the code to run on a GPU.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2. Training a classifier with PyTorch\n",
    "\n",
    "Now that we are introduced PyTorch tensors, we will look at how to use PyTorch to train neural networks. We will do the following steps:\n",
    "\n",
    "1. Load data \n",
    "2. Define a two-layer network\n",
    "3. Define a loss function and optimizer\n",
    "4. Train the network\n",
    "5. Test the network\n",
    "\n",
    "## 2.1 Loading Datasets\n",
    "\n",
    "The general procedure of loading data is :\n",
    "- Extract data from  source\n",
    "- Transform the data into a suitable form (for example, to a Tensor)\n",
    "- Put our data into an object to make it easy to access further on\n",
    "\n",
    "### 2.1.1 Loading the Housing Price dataset\n",
    "\n",
    "We'll use both our `DataLoader` class from the previous exercises and PyTorch's `DataLoader` to load the house price dataset that we used in Exercise 4 to classify the price of the houses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us fetch the data and setup the `Dataset` class as in Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.data.csv_dataset import CSVDataset, get_exercise5_transform\n",
    "from exercise_code.data.dataloader import DataLoader as our_DataLoader\n",
    "\n",
    "# dataloading and preprocessing steps as in ex04 \n",
    "target_column = 'SalePrice'\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "root_path = os.path.join(i2dl_exercises_path, \"datasets\", 'housing')\n",
    "housing_file_path = os.path.join(root_path, \"housing_train.csv\")\n",
    "download_url = 'https://i2dl.vc.in.tum.de/static/data/housing_train.zip'\n",
    "\n",
    "# Set up the transform to get two prepared columns\n",
    "select_two_columns_transform = get_exercise5_transform()\n",
    "\n",
    "# Set up the dataset\n",
    "our_csv_dataset = CSVDataset(target_column=target_column, root=root_path, download_url=download_url, mode=\"train\",\n",
    "                             transform=select_two_columns_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now set our `DataLoader` class to help us to load batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "\n",
      "Dictionary Key: features\n",
      "Value Type <class 'numpy.ndarray'>\n",
      "Shape of the Value (4, 2)\n",
      "\n",
      "Dictionary Key: target\n",
      "Value Type <class 'numpy.ndarray'>\n",
      "Shape of the Value (4, 1)\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the first batch of the data\n",
    "batch_size = 4\n",
    "our_dataloader = our_DataLoader(our_csv_dataset, batch_size=batch_size)\n",
    "\n",
    "for i, item in enumerate(our_dataloader):\n",
    "    print('Batch {}'.format(i))\n",
    "    for key in item:\n",
    "        print(\"\\nDictionary Key:\",key)\n",
    "        print(\"Value Type\",type(item[key]))\n",
    "        print(\"Shape of the Value\",item[key].shape)\n",
    "    \n",
    "    if i+1 >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In PyTorch we can use the [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class to accomplish the same objective. It provides more parameters than our `DataLoader` class, such as easy multiprocessing using `num_workers`. You can refer the documentation to learn those additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "\n",
      "Dictionary Key: features\n",
      "Value Type <class 'torch.Tensor'>\n",
      "Shape of the Value torch.Size([4, 2])\n",
      "\n",
      "Dictionary Key: target\n",
      "Value Type <class 'torch.Tensor'>\n",
      "Shape of the Value torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "pytorch_dataloader = DataLoader(our_csv_dataset, batch_size=batch_size)\n",
    "\n",
    "# We can use the exact same way to iterate over samples\n",
    "for i, item in enumerate(pytorch_dataloader):\n",
    "    print('Batch {}'.format(i))\n",
    "    for key in item:\n",
    "        print(\"\\nDictionary Key:\",key)\n",
    "        print(\"Value Type\",type(item[key]))\n",
    "        print(\"Shape of the Value\",item[key].shape)\n",
    "    \n",
    "    if i+1 >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">As seen above, both the data loaders load the data with the same batch size and the data contains 2 features and 1 target. The only difference here is that PyTorch's <code>DataLoader</code> will automatically transform the dataset into <b>Tensor</b> data type.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Torchvision\n",
    "\n",
    "Specifically for computer vision, the `torchvision` packages has data loaders for many common datasets such\n",
    "as ImageNet, FashionMNIST, MNIST and additional data transformers for images in `torchvision.datasets` and `torch.utils.data.DataLoader` modules.\n",
    "\n",
    "This is highly convenient and is useful in avoiding  to write boilerplate code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try loading the [`Fashion-MNIST`](https://research.zalando.com/welcome/mission/research-projects/fashion-mnist/) dataset. It has  gray-scale images of size $28* 28$ belonging to 10 different classes of clothing accessories such as T-Shirt, Trousers, Sneakers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transforms.Compose` creates a series of transformation to prepare the dataset.\n",
    "- `transforms.ToTensor` convert `PIL image` or numpy.ndarray $(H \\times W\\times C)$ in the range [0,255] to a `torch.FloatTensor` of shape $(C \\times H \\times W)$ in the range [0.0, 1.0].\n",
    "\n",
    "- `transforms.Normalize` normalize a tensor image with the provided mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and standard deviations have to be sequences (e.g. tuples),hence we add a comma after the values\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,),(0.5,))]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`datasets.FashionMNIST` downloads the Fashion MNIST dataset and transforms it using our previous cell definition.  \n",
    "By setting the value of `train`, we get the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../datasets/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d687cf4cd4e64d1d8d1f22bb961c476e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../datasets/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../datasets/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d71a975f2794a84b66c73441a927af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../datasets/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../datasets/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb79975251f4434b368f9e0b7871783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../datasets/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../datasets/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce46a454353448a8c9ea17c40ae954c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../datasets/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../datasets/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist_dataset = torchvision.datasets.FashionMNIST(root='../datasets', train=True,\n",
    "                                                          download=True, transform=transform)\n",
    "fashion_mnist_test_dataset = torchvision.datasets.FashionMNIST(root='../datasets', train=False,\n",
    "                                                          download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " `torch.utils.data.Dataloader` takes our training data or test data with parameter\n",
    "`batch_size` and `shuffle`. The variable `batch_size` defines how many samples per batch to load. The variable `shuffle=True` makes the data reshuffled at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist_dataloader = DataLoader(fashion_mnist_dataset, batch_size=8)\n",
    "fashion_mnist_test_dataloader = DataLoader(fashion_mnist_test_dataset, batch_size=8)\n",
    "\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first batch of data from the `fashion_mnist_dataloader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Datatype of Image: <class 'torch.Tensor'>\n",
      "Shape of the Image: torch.Size([8, 1, 28, 28])\n",
      "Label Values: tensor([9, 0, 0, 3, 0, 2, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "# We can use the exact same way to iterate over samples\n",
    "for i, item in enumerate(fashion_mnist_dataloader):\n",
    "    print('Batch {}'.format(i))\n",
    "    image, label = item\n",
    "    print(f\"Datatype of Image: {type(image)}\")\n",
    "    print(f\"Shape of the Image: {image.shape}\")\n",
    "    print(f\"Label Values: {label}\")\n",
    "\n",
    "    if i+1 >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since we loaded the data with `batch_size` 8, the shape of the input is (8, 1, 28, 28). \n",
    "\n",
    "Let's look at  some of the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ankle boot T-shirt/top T-shirt/top Dress T-shirt/top Pullover Sneaker Pullover\n"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5 # unormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(fashion_mnist_dataloader)\n",
    "for images, labels in fashion_mnist_dataloader:\n",
    "    break\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Defining the Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%% md\n"
    }
   },
   "source": [
    "We implemented the `ClassificationNet` class in Exercise 06. Let's use it here again for Fashion-MNIST.\n",
    "\n",
    "Have a look at our lengthy implementation first/again to appreciate the upcoming shortness ;)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks.classification_net import ClassificationNet\n",
    "hidden_size = 100\n",
    "std = 1.0\n",
    "model_ex06 = ClassificationNet(num_layer=2,input_size=1*28*28, hidden_size=hidden_size, std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides a `nn.Module` that builds neural networks. Now, we will use it to define our network class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, activation=nn.Sigmoid(),\n",
    "                 input_size=1*28*28, hidden_size=100, classes=10):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Here we initialize our activation and set up our two linear layers\n",
    "        self.activation = activation\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size) # flatten\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Looking at the constructor of `Net`, we have,\n",
    " - `super().__init__` creates a class that inherits attributes and behaviors from another class.\n",
    "\n",
    " - `self.fc1` creates an affine layer with `input_size` inputs and `hidden_size` outputs.\n",
    "\n",
    " - `self.fc2` is the second affine layer.\n",
    "\n",
    "The `Forward` function defines the forward pass of the mode.:\n",
    "\n",
    " - Input `x` is flattened with `x = x.view(-1, self.input_size)` to be able to use as input to the affine layer.\n",
    "\n",
    " - Apply `fc1`, `activation`, `fc2` sequentially to complete the network.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Central to all neural networks in PyTorch is the [`autograd`](https://pytorch.org/docs/stable/autograd.html) package. It provides automatic differentiation for all operations on Tensors. \n",
    "If we set the attribute `.requires_grad` of `torch.Tensor` as `True`, it tracks all operations applied on that tensor. Once all the computations are finished, the function `.backward()` computes the gradients into the `Tensor.grad` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Thanks to the <b>autograd</b> package, we just have to define the <b>forward()</b> function. We can use any of the Tensor operations in the <b>forward()</b>  function.\n",
    " The <b>backward()</b> function (where gradients are computed through back-propagation) is automatically defined by PyTorch.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `print()` to look at all the defined layers of the network (but it won't show the information of the forward pass).\n",
    "\n",
    "The learned parameters of a model are returned by `[model_name].parameters()`. We can also access the parameters of different layers by `[model_name].[layer_name].parameters()`.\n",
    "\n",
    "Let's create an instance of the `Net` model and look at the parameters matrix shape for each of the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (activation): Sigmoid()\n",
      "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "Shapes of the Parameter Matrix:\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "# Always remember to move the network to the GPU/CPU depending on device\n",
    "net = net.to(device) \n",
    "\n",
    "print(net)\n",
    "\n",
    "print(\"Shapes of the Parameter Matrix:\")\n",
    "for parameter in net.parameters():\n",
    "        print(parameter.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.3 Defining the Loss function and optimizer\n",
    "\n",
    "Since it is a multi-class classification, we will use the Cross-Entropy loss and optimize it using SGD with momentum. We had implemented SGD with momentum in Exercise 05. Have a look at the implementations in `exercise_code/networks/optimizer.py` and `exercise_code/networks/loss.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from exercise_code.networks.optimizer import sgd_momentum\n",
    "from exercise_code.networks.loss import CrossEntropyFromLogits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `torch.nn` and `torch.optim` modules include a variety of loss functions and optimizers. We will initialize an instance of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.4 Training the network\n",
    "\n",
    "We have completed setting up the dataloader, loss function as well as the optimizer. We are now all set for training the network.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1, Iteration  1000] loss: 1.526 acc: 56.49 %\n",
      "[Epoch 1, Iteration  2000] loss: 0.900 acc: 72.28 %\n",
      "[Epoch 1, Iteration  3000] loss: 0.738 acc: 74.54 %\n",
      "[Epoch 1, Iteration  4000] loss: 0.654 acc: 77.05 %\n",
      "[Epoch 1, Iteration  5000] loss: 0.609 acc: 78.89 %\n",
      "[Epoch 1, Iteration  6000] loss: 0.576 acc: 79.71 %\n",
      "[Epoch 1, Iteration  7000] loss: 0.555 acc: 80.21 %\n",
      "[Epoch 2, Iteration  1000] loss: 0.522 acc: 81.65 %\n",
      "[Epoch 2, Iteration  2000] loss: 0.508 acc: 82.10 %\n",
      "[Epoch 2, Iteration  3000] loss: 0.510 acc: 81.94 %\n",
      "[Epoch 2, Iteration  4000] loss: 0.487 acc: 82.99 %\n",
      "[Epoch 2, Iteration  5000] loss: 0.484 acc: 83.46 %\n",
      "[Epoch 2, Iteration  6000] loss: 0.472 acc: 83.54 %\n",
      "[Epoch 2, Iteration  7000] loss: 0.472 acc: 83.21 %\n",
      "FINISH.\n"
     ]
    }
   ],
   "source": [
    "# Initializing the list for storing the loss and accuracy\n",
    "\n",
    "train_loss_history = [] # loss\n",
    "train_acc_history = [] # accuracy\n",
    "\n",
    "for epoch in range(2):\n",
    "\n",
    "       \n",
    "    running_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "    \n",
    "    # Iterating through the minibatches of the data\n",
    "    \n",
    "    for i, data in enumerate(fashion_mnist_dataloader, 0):\n",
    "        \n",
    "        # data is a tuple of (inputs, labels)\n",
    "        X, y = data\n",
    "\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Reset the parameter gradients  for the current  minibatch iteration \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        y_pred = net(X)             # Perform a forward pass on the network with inputs\n",
    "        loss = criterion(y_pred, y) # calculate the loss with the network predictions and ground Truth\n",
    "        loss.backward()             # Perform a backward pass to calculate the gradients\n",
    "        optimizer.step()            # Optimize the network parameters with calculated gradients\n",
    "\n",
    "        \n",
    "        # Accumulate the loss and calculate the accuracy of predictions\n",
    "        running_loss += loss.item()\n",
    "        _, preds = torch.max(y_pred, 1) #convert output probabilities of each class to a singular class prediction\n",
    "        correct += preds.eq(y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "        # Print statistics to console\n",
    "        if i % 1000 == 999: # print every 1000 mini-batches\n",
    "            running_loss /= 1000\n",
    "            correct /= total\n",
    "            print(\"[Epoch %d, Iteration %5d] loss: %.3f acc: %.2f %%\" % (epoch+1, i+1, running_loss, 100*correct))\n",
    "            train_loss_history.append(running_loss)\n",
    "            train_acc_history.append(correct)\n",
    "            running_loss = 0.0\n",
    "            correct = 0.0\n",
    "            total = 0\n",
    "\n",
    "print('FINISH.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So the general training pass is summarized below:\n",
    "\n",
    "- `zero_grad()`: Zero the gradient buffers of all the model parameters to start the current minibatch iteration.\n",
    "\n",
    "- `y_pred = net(X)`: Make a forward pass through the network by passing the images to the model to get the predictions, which are log probabilities of image belonging to each of the class.\n",
    "\n",
    "- `loss = criterion(y_pred, y)`: Calculate the loss from the generated predictions and the training data `y`.\n",
    "\n",
    "- `loss.backward()`: Perform a backward pass through the network to calculate the gradients for model parameters.\n",
    "\n",
    "- `optimizer.step()`: Do an optimization step to update the model parameters using the calculated gradients.\n",
    "\n",
    "We keep tracking the training loss and accuracy over time. The following plot shows average values for train loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(train_acc_history)\n",
    "plt.plot(train_loss_history)\n",
    "plt.title(\"FashionMNIST\")\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('acc/loss')\n",
    "plt.legend(['acc', 'loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Testing the performance of the model\n",
    "\n",
    "We have trained the network for 2 passes over the entire training dataset. Let's check the model performance using the test data.\n",
    "We will pass the test data to the model to predict the class label and check it against the ground-truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of test images\n",
    "dataiter = iter(fashion_mnist_test_dataloader)\n",
    "images, labels = dataiter.__next__()\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "# get sample outputs\n",
    "outputs = net(images)\n",
    "# convert output probabilites to predicted class\n",
    "_, predicted = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "We will visualize the results to display the test images and their labels in the following format: `predicted (ground-truth)`. The text will be green for accurately classified examples and red for incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep images for display\n",
    "if not isinstance(images, np.ndarray):\n",
    "    images = images.cpu().numpy()\n",
    "\n",
    "# plot the images in the batch, along with predicted and true labels\n",
    "fig = plt.figure(figsize=(25,4))\n",
    "for idx in range(8):\n",
    "    ax = fig.add_subplot(2, 8//2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(f\"{classes[predicted[idx]]} ({classes[labels[idx]]})\",\n",
    "                color=\"green\" if predicted[idx]==labels[idx] else \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's find which classes of images performed well, and the classes that did not perform well!  \n",
    "`torch.no_grad()` makes sure that gradients are not calculated for the tensors since we only are performing a forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in fashion_mnist_test_dataloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %11s: %2d %%' % (classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the end of the `PyTorch` Tutorial. In the next notebook, we will look at [`TensorBoard`](https://www.tensorflow.org/tensorboard) which helps us visualize the results of the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "1. [PyTorch Tutorial](https://pytorch.org/tutorials/)\n",
    "\n",
    "2. [Fashion MNIST dataset training using PyTorch](https://medium.com/@aaysbt/fashion-mnist-data-training-using-pytorch-7f6ad71e96f4)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "54970da6898dad277dbf355945c2dee7f942d2a31ec1fc1455b6d4f552d07b83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
